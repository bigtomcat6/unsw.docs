---
title: 2.5 Jointly distributed random variables
createTime: 2024/09/03 16:56:56
permalink: /MATH2099/pqfyjtj3/
---

## Question 1

<div class="how_qb">

**Concept**

<iframe width="672" height="378" src="https://www.youtube.com/embed/CXjqgtEdLkc" title="L2 26 Jointly Distributed Random Variables" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Video Recap

- The **joint cumulative distribution function** of $X$ and $Y$ is given by $F_{XY}(x,y)=\mathbb{P}(X\leq x,Y\leq y)$ for all $(x,y) \in \mathbb{R} \times \mathbb{R}$.
- If $X$ and $Y$ are both discrete,
  - the joint probability mass function is defined by $p_{XY}(x,y)=\mathbb{P}(X=x,Y=y)$,
  - The marginal pmf of $X$ and $Y$ can be obtained by $\displaystyle p_X(x)=\sum_{y \in S_Y} p_{XY}(x,y)$ and $\displaystyle p_Y(y)=\sum_{x \in S_X} p_{XY}(x,y)$.
  - For any function $g:\mathbb{R}\times \mathbb{R} \to \mathbb{R}$, the expectation of $g(X,Y)$ is given by $\displaystyle\mathbb{E}(g(X,Y)) = \sum_{x\ \in S_X} \sum_{y \in S_Y} g(x,y) p_{XY}(x,y)$ in the discrete case.
- If $X$ and $Y$ are both continuous, analogous results apply.
- $\mathbb{E}(aX+bY) = a\mathbb{E}(X)+b\mathbb{E}(Y)$.

</div>

## Question 2

<div class="how_qb">

**Example**

Use results for linear combinations of random variables to compute the expected sum obtained when two fair dice are rolled.

`7` *(Enter the exact value)*

<iframe width="672" height="378" src="https://www.youtube.com/embed/hOv7UI0Fz1E" title="L2 27 Jointly Distributed Example" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

</div>


## Question 3

<div class="how_qb">

**Concept**

<iframe width="672" height="378" src="https://www.youtube.com/embed/LqbRPG7s6rU" title="L2 28 Independent Random Variables" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
Video Recap

- The random variables $X$ and $Y$ are said to be **independent** if $\mathbb{P}(X \leq x,Y\leq y)=\mathbb{P}(X \leq x)\times \mathbb{P}(Y \leq y)$ for all $(x,y) \in \mathbb{R} \times \mathbb{R}$.
- X and Y are independent if and only if for any $(x,y) \in \mathbb{R} \times \mathbb{R}$, $F_{XY}(x,y)=F_X(x)\times F_Y(y)$.
- If $X$ and $Y$ are **independent**, then $\mathbb{E}(h(X)g(Y))=\mathbb{E}(h(X))\times \mathbb{E}(g(Y))$ for any functions $h$ and $g$.

</div>